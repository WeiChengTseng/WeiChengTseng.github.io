<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-P8YEESN55E"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'G-P8YEESN55E');
    </script>

    <link rel="shortcut icon" href="./assets/favicon.ico">
    <meta name="description" content="Offline Multi-Agent Reinforcement Learning with Knowledge Distillation.">
    <meta name="keywords" content="offline reinforcement learning, RL, multi-agent reinforcement learning">
    <title>Offline-MARL</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
    <style>
        /* Remove the navbar's default margin-bottom and rounded borders */
        
        .navbar {
            margin-bottom: 0;
            border-radius: 0;
        }
        /* Add a gray background color and some padding to the footer */
        
        footer {
            background-color: #f2f2f2;
            padding: 25px;
        }
    </style>
    <link rel="stylesheet" href="./assets/font.css">
    <link rel="stylesheet" href="./assets/main.css">
</head>

<body>

    <div class="jumbotron">
        <div class="container text-center">
            <!-- <h1 style="color:white;margin-bottom:0;">CLA-NeRF</h1> -->
            <h2 style="color:white;margin-top:10;">Offline Multi-Agent Reinforcement Learning with Knowledge Distillation</h3>
            <br>
            <p style="color:white"><a>NeurIPS 2022</a><br>
                <a>Submission</a>
                <br>
        </div>
    </div>

    <div class="container bg-3">
        <div class="row">
            <h2 class="text-center">Abstract</h2>
            <hr />
            <!-- <div class="col-sm-10 col-sm-offset-1 text-center">
                <img src="./assets/teaser_v7.png" id="teaser" class="img-responsive" alt="Image">
            </div> -->
        </div>
        <br>
        <p>We introduce a offline multi-agent reinforcement learning (MARL) framework that utilizes previously collected data without additional online data collection. Our method reformulates offline MARL as a sequence modeling problem and thus builds on top of the simplicity and scalability of the Transformer architecture. In the fashion of centralized training and decentralized execution, we propose to first train a teacher policy as if the MARL dataset is generated by a single agent. After the teacher policy has identified and recombined the "good" behavior in the dataset, we create separate student policies and distill not only the teacher policy's features but also its structural relations among different agents' features to student policies. Despite its simplicity, the proposed method outperforms state-of-the-art model-free offline MARL baselines while being more robust to demonstration's quality on several environments.
        </p>
    </div><br><br>

    <div class="container bg-3">
        <div class="row">
            <h2 class="text-center">Overview</h2>
            <hr />
            <!-- <div class="col-sm-10 col-sm-offset-1 text-center">
                <img src="./assets/teaser_v7.png" id="teaser" class="img-responsive" alt="Image">
            </div> -->
        </div>
        <br>
        <p>
            <!-- use seperate figure to subfigure and describe them separatedly -->
            We compare (a) independent decision transformer (IDT), (b) multi-agent decision transformer(MADT), and (c) our approach. (a) IDT trains an independent decision transformer for each agent separately. (b) MADT extends IDT by sharing parameters across multiple agents and concatenate agent's one-shot IDs to the observations. (c) Our approach first train a teacher policy, instantiated by a centralized decision transformer, and then distill both its features and structural relations among features to IDT.
            <!-- (a) The goal of this task is to explore as many blocks as possible with multiple agents. Explored blocks are colored (e.g., green). (b) Training dataset consists of agents' random walk trajectories and their per-step rewards. (c) Agents trained with behavior cloning result in suboptimal performance due to the low-quality demonstrations in the training dataset. (d) A centralized Decision Transformer (Teacher DT) can condition on high reward to achieve superior performance. However, it cannot be deployed during test time due to its assumption to access every agent's information. (e) To fix this, we propose a framework to distill the centralized Decision Transformer's policy into multiple student policies (Student DT) and show that it achieve superior performance under decentralized execution. -->
        </p>
            <div class="col-sm-10 col-sm-offset-1 text-center">
                <img src="./assets/teaser_v3.png" id="teaser" class="img-responsive" alt="Image">
            </div>
    </div><br><br>

    <div class="container bg-3">
        <br>

        <div class="row">
            <h2 class="text-center">Quantative Results</h2>
            <hr />

            <div class="col-sm-12 text-center">
                <img src="./assets/env_row.png" class="img-responsive center" style="width:100%" alt="Image">
            </div>
            <p>
                <!-- <b>Fill-In</b>:  The goal of this task is to explore as many blocks as possible with multiple agents. Explored blocks are colored (e.g., green). Training dataset consists of agents' random walk trajectories and their per-step rewards.
                <br>
                &emsp;- <b>Ours</b>: perform the best<br>
                &emsp;- <b>BC</b>: mimic the random walks, which perform the worst<br>
                &emsp;- <b>MADT</b>: performs better than IDT and BC by filling more blocks, but still can’t outperform our approach<br>
                &emsp;- <b>IDT</b>: agents somehow understand
                how to fill the blocks. However, the trajectories of the agents have some overlap. -->


                <!-- <b>Deformation </b> We test our model We deform the object with predicted joint information and part segmentation. These results show the objects are deformed consistently for both synthetic and real scenario. -->
            </p>
            <div class="col-sm-12 text-center">
                <img src="./assets/quantative_all.png" class="img-responsive center" style="width:80%" alt="Image">
            </div>
        </div>

        <!-- <hr />
        <p>
            <b>Equal Space </b> a particle system environment that agents need to keep the same distance between each other.
        </p>
        <div class="col-sm-12 text-center"> -->
            <!-- <img src="./assets/ape.gif" class="img-responsive" style="width:120%" alt="Image"> -->
        <!-- </div> -->
    </div><br><br>    

    <div class="container bg-3">
        <br>

        <div class="row">
            <h2 class="text-center">Qualtative Results</h2>
            <hr />
            <p>
                <b>Fill-In</b>:  The goal of this task is to explore as many blocks as possible with multiple agents. Explored blocks are colored (e.g., green). Training dataset consists of agents' random walk trajectories and their per-step rewards.
                <br>
                &emsp;- <b>Ours</b>: perform the best<br>
                &emsp;- <b>BC</b>: mimic the random walks, which perform the worst<br>
                &emsp;- <b>MADT</b>: performs better than IDT and BC by filling more blocks, but still can’t outperform our approach<br>
                &emsp;- <b>IDT</b>: agents somehow understand
                how to fill the blocks. However, the trajectories of the agents have some overlap.


                <!-- <b>Deformation </b> We test our model We deform the object with predicted joint information and part segmentation. These results show the objects are deformed consistently for both synthetic and real scenario. -->
            </p>
            <div class="col-sm-12 text-center">
                <img src="./assets/fill-in-demo.gif" class="img-responsive center" style="width:80%" alt="Image">
            </div>
        </div>

        <!-- <hr />
        <p>
            <b>Equal Space </b> a particle system environment that agents need to keep the same distance between each other.
        </p>
        <div class="col-sm-12 text-center">
            <img src="./assets/ape.gif" class="img-responsive" style="width:120%" alt="Image">
        </div> -->
    </div><br><br>


    <!-- <div class="container bg-3">
        <br>

        <div class="row">
            <h2 class="text-center">Synthetic Results</h2>
            <hr />
            <p>
                <b>Articulated View Systhesis</b>: We directly test our model on holdout objects generated with CAD models. Here, we visulaize novel view synthesis, part segmentation, and deformarion results with predicted joint attributes.


                <!-- <b>Deformation </b> We test our model We deform the object with predicted joint information and part segmentation. These results show the objects are deformed consistently for both synthetic and real scenario. -->
            <!-- </p> -->
            <!-- <div class="col-sm-12 text-center"> -->
                <!-- <img src="./assets/final_deform.gif" class="img-responsive" style="width:120%" alt="Image"> -->
            <!-- </div> -->
        </div>

        <!-- <hr />
        <p>
            <b>Articulated Pose Estimation </b> We show overlaid images of the rendered and observed images during the optimization of the articulated pose. These examples show that CLA-NeRF is able to recover real setting.
        </p> -->
    </div><br><br> -->



</body>

</html>