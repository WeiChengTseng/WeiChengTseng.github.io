<!DOCTYPE html>
<html lang="en">

<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-P8YEESN55E"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'G-P8YEESN55E');
    </script>

    <link rel="shortcut icon" href="./assets/favicon.ico">
    <meta name="description" content="CLA-NeRF: Category-Level Articulated Neural Radiance Field.">
    <meta name="keywords" content="CLA-NeRF,NeRF,Articulated Pose Estimation,Differentianle,Rendering,Neural">
    <title>CLA-NeRF</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>


    <!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
		integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
	<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic'
		rel='stylesheet' type='text/css'>
	<link href="https://fonts.googleapis.com/css2?family=Courier+Prime&family=Open+Sans&family=Roboto&display=swap"
		rel="stylesheet"> -->
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" />
    <style>
        /* Remove the navbar's default margin-bottom and rounded borders */

        .navbar {
            margin-bottom: 0;
            border-radius: 0;
        }

        /* Add a gray background color and some padding to the footer */

        footer {
            background-color: #f2f2f2;
            padding: 25px;
        }
    </style>
    <!-- <link rel="stylesheet" type="text/css" href="style.css"> -->
    <link rel="stylesheet" href="./assets/font.css">
    <link rel="stylesheet" href="./assets/main.css">
</head>

<body>

    <div class="jumbotron">
        <div class="container text-center">
            <h1 style="color:white;margin-bottom:0;">CLA-NeRF</h1>
            <h3 style="color:white;margin-top:0;">Category-Level Articulated Neural Radiance Field</h3>
            <br>
            <p style="color:white"><a>ICRA 2022</a><br>
                <a href="https://weichengtseng.github.io/">Wei-Cheng Tseng</a>, <a href="https://hankliao87.github.io/">Hung-Ju Liao</a>, <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, <a href="https://aliensunmin.github.io/">Min Sun</a>
                <br>
        </div>
    </div>

    <!-- <div class="container bg-3">
        <div class="row">
            <h2 class="text-center">Overview</h2>
            <hr />
            <div class="col-sm-10 col-sm-offset-1 text-center">
                <img src="./assets/teaser_v7.png" id="teaser" class="img-responsive" alt="Image">
            </div>
        </div>
        <br>
        <p>We present a framework that takes a few visual observations and corresponding camera poses as input; then, we can perform (a) view synthesis and (b) part segmentation from unseen viewpoints and articulated poses. Moreover, (c) the articulated
            pose can be estimated via inversely optimizing the 3D deformation through our framework to match the target visual observation.
        </p>
    </div><br><br> -->

    <div class="container bg-3">
        <br>

        <div class="row">
            <h2 class="text-center">Overview</h2>
            <hr />
            <div class="col-sm-12 text-center">
                <img src="./assets/teaser.gif" class="img-responsive" style="width:120%" alt="Image">
            </div>
            <div class="container spacing">
            </div>
            <div class="container spacing">
            </div>
            <p>
                We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field that can perform view
                synthesis, part segmentation, and articulated pose estimation. CLA-NeRF is trained at the object
                category level using no CAD models and no depth, but a set of RGB images with ground truth camera poses
                and part segments. During inference, it only takes a few RGB views (i.e., few-shot) of an unseen 3D
                object instance within the known category to infer the object part segmentation and the neural radiance
                field. Given an articulated pose as input, CLA-NeRF can perform articulation-aware volume rendering to
                generate the corresponding RGB image at any camera pose. Moreover, the articulated pose of an object can
                be estimated via inverse rendering. In our experiments, we evaluate the framework across five categories
                on both synthetic and real-world data. In all cases, our method shows realistic deformation results and
                accurate articulated pose estimation. We believe that both few-shot articulated object rendering and
                articulated pose estimation open doors for robots to perceive and interact with unseen articulated
                objects.
            </p>
        </div>
    </div><br><br>

    <div class="container bg-3">
        <br>
    <div class="row">
        <h2 class="text-center">Method</h2>
        <hr />
        <div class="col-sm-12 text-center">
            <img src="./assets/overview_v7.png" class="img-responsive" style="width:100%" alt="Image">
        </div>
        <div class="container spacing">
        </div>
        <div class="container spacing">
        </div>
        <p>
            (a) Our framework retrieves features from two instance as the condition of NeRF model and predicts color <b>c</b>, density <b>&sigma;</b> and segmentation <b>s</b>. The volume rendering is applied to predict rendered results.
            (b) We calculate the deformation matrix based on the articulated pose. Then, we deform the sampled rays with the deformation matrix. Finally, the deformed visual image is rendered using our learned framework. 
            (c) The articulated pose is estimated via inversely minimizing <b>L<sub>color</sub></b>.
        </p>
    </div>
</div><br><br>



    <div class="container bg-3">
        <br>
        <div class="row">
            <h2 class="text-center">Real-World Results</h2>
            <hr />
            <p>
                <b>Articulated View Systhesis</b>: We directly test our model on real-world images without finetuning.
                Here, we visulaize novel view synthesis, part segmentation, and deformarion results with predicted joint
                attributes.


                <!-- <b>Deformation </b> We test our model We deform the object with predicted joint information and part segmentation. These results show the objects are deformed consistently for both synthetic and real scenario. -->
            </p>
            <div class="col-sm-12 text-center">
                <img src="./assets/real_deform_mod.gif" class="img-responsive" style="width:120%" alt="Image">
            </div>
        </div>

        <hr />
        <p>
            <b>Articulated Pose Estimation </b> We show overlaid images of the rendered and observed images during the
            optimization of the articulated pose. These examples show that CLA-NeRF is able to recover real setting.
        </p>
        <div class="col-sm-12 text-center">
            <img src="./assets/ape.gif" class="img-responsive" style="width:120%" alt="Image">
        </div>
    </div>
    <br><br>


    <div class="container bg-3">
        <br>

        <div class="row">
            <h2 class="text-center">Synthetic Results</h2>
            <hr />
            <p>
                <b>Articulated View Systhesis</b>: We directly test our model on holdout objects generated with CAD
                models. Here, we visulaize novel view synthesis, part segmentation, and deformarion results with
                predicted joint attributes.


                <!-- <b>Deformation </b> We test our model We deform the object with predicted joint information and part segmentation. These results show the objects are deformed consistently for both synthetic and real scenario. -->
            </p>
            <div class="col-sm-12 text-center">
                <img src="./assets/final_deform.gif" class="img-responsive" style="width:120%" alt="Image">
            </div>
        </div>

        <!-- <hr />
        <p>
            <b>Articulated Pose Estimation </b> We show overlaid images of the rendered and observed images during the optimization of the articulated pose. These examples show that CLA-NeRF is able to recover real setting.
        </p> -->
    </div><br><br>


    <div class="container bg-3">
        <br>

        <div class="row">
            <h2 class="text-center">Resource and Citation</h2>
            <hr />

            <img class="paper_snapshot" src="./assets/all.png">
            <div class="container spacing">
            </div>
            <a type="button" class="btn btn-light" href="https://www.youtube.com/watch?v=_AmEgYYnVJA"><i class="fa fa-video-camera"
                    aria-hidden="true"></i> Video </a>&nbsp
            <a type="button" class="btn btn-light" href="https://arxiv.org/abs/2202.00181"><i class="fa fa-file-pdf-o"
                    aria-hidden="true"></i> Paper </a>&nbsp
            <a type="button" class="btn btn-light" href="https://github.com/WeiChengTseng/CLA-NeRF"><i
                    class="fa fa-github-alt" aria-hidden="true"></i> Code (comming soon) </a>&nbsp
            <a type="button" class="btn btn-light" href="#"><i class="fa fa-database"></i> Dataset (comming soon)
            </a>&nbsp
            <div class="container spacing">
            </div>
            <div class="bitex">
                <code>
					&ensp; @inproceedings{icra_tseng,<br />
					&ensp; &nbsp &nbsp author = {Wei-Cheng Tseng, Hung-Ju Liao, Lin Yen-Chen, Min Sun},<br />
					&ensp; &nbsp &nbsp title = {CLA-NeRF: Category-Level Articulated Neural Radiance Field},<br />
					&ensp; &nbsp &nbsp journal = {ICRA},<br />
					&ensp; &nbsp &nbsp year = {2022}<br />
					&ensp; }
				</code>
            </div>
        </div>

        <!-- <hr />
        <p>
            <b>Articulated Pose Estimation </b> We show overlaid images of the rendered and observed images during the optimization of the articulated pose. These examples show that CLA-NeRF is able to recover real setting.
        </p> -->
    </div><br><br>

    <!-- <div class="container bg-3">
        <div class="row">
            <h2 class="text-center">Abstract</h2>
            <hr />
        </div>
        <br>
        <p>We propose CLA-NeRF - a Category-Level Articulated Neural Radiance Field that can perform view synthesis, part segmentation, and articulated pose estimation. CLA-NeRF is trained at the object category level using no CAD models and no depth, but
            a set of RGB images with ground truth camera poses and part segments. During inference, it only takes a few RGB views (i.e., few-shot) of an unseen 3D object instance within the known category to infer the object part segmentation and the
            neural radiance field. Given an articulated pose as input, CLA-NeRF can perform articulation-aware volume rendering to generate the corresponding RGB image at any camera pose. Moreover, the articulated pose of an object can be estimated via
            inverse rendering. In our experiments, we evaluate the framework across three categories (i.e. laptop, scissors, and eyeglasses) on both synthetic and real-world data. In all cases, our method shows realistic deformation results and accurate
            articulated pose estimation. We believe that both few-shot articulated object rendering and articulated pose estimation open doors for robots to perceive and interact with unseen articulated objects.
        </p>
    </div><br><br> -->

    <!-- <div class="container bg-3">
        <div class="row">
            <h2 class="text-center">Method</h2>
            <hr />
            <div class="col-sm-10 col-sm-offset-1 text-center">
                <img src="./assets/overview_v7.png" id="teaser" class="img-responsive" style="width:120%" alt="Image">
            </div>
        </div>
        <br>
        <p>The overview of our framework. (a) Our framework retrieves features from two instance as the condition of NeRF model and predicts color <b>c</b>, density <math><b>&sigma;</b></math> and segmentation <math><b>s</b></math>. The volume rendering
            is applied to predict rendered results. (b) We calculate the deformation matrix based on the articulated pose. Then, we deform the sampled rays with the deformation matrix. Finally, the deformed visual image is rendered using our learned framework.
            (c) The articulated pose is estimated via inversely minimizing <math><b>L<sub>color</sub></b></math>.
        </p>
    </div><br><br> -->


</body>

</html>